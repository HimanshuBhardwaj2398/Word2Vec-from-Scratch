{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential \n",
    "from tensorflow.keras.layers import Input, Dense, Reshape,Embedding,dot\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, let us consider the sentence from the lecture \n",
    "raw_data = \"Rohan was hit by a red bus, so Kunal rushed to the scene with a red scarf, \\\n",
    "but Chris and Marios did not rush to the scene and instead watched a movie\"\n",
    "\n",
    "#Conver the raw_data into a list of words separated by spaces\n",
    "corpus = raw_data.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'red': 1,\n",
       " 'to': 2,\n",
       " 'the': 3,\n",
       " 'scene': 4,\n",
       " 'and': 5,\n",
       " 'rohan': 6,\n",
       " 'was': 7,\n",
       " 'hit': 8,\n",
       " 'by': 9,\n",
       " 'bus,': 10,\n",
       " 'so': 11,\n",
       " 'kunal': 12,\n",
       " 'rushed': 13,\n",
       " 'with': 14,\n",
       " 'scarf,': 15,\n",
       " 'but': 16,\n",
       " 'chris': 17,\n",
       " 'marios': 18,\n",
       " 'did': 19,\n",
       " 'not': 20,\n",
       " 'rush': 21,\n",
       " 'instead': 22,\n",
       " 'watched': 23,\n",
       " 'movie': 24}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=collections.Counter(corpus).most_common(len(corpus))\n",
    "dictionary={}\n",
    "for i,word in enumerate(count):\n",
    "        dictionary[word[0]]=i\n",
    "## checking the dictionary\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `build_dataset(text_corpus, vocabulary_size)`\n",
    "\n",
    "We want to write a function, that takes the list of words, and your vocabulary size and returns\n",
    "1. Tokenized sequences\n",
    "2. Count of each word\n",
    "3. Dictionary to convert word to token\n",
    "4. Reverse dictionary to convert token to word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(text_corpus, vocabulary_size):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    \n",
    "    # The 'UNK' tag is a special character that is assigned token value zero. It represents 'out of vocabulary words'\n",
    "    # We initialize it to a random value to begin with\n",
    "    \n",
    "    count = [['UNK', -1]]\n",
    "    \n",
    "    # For all words in sequence, get the count of the most common words\n",
    "    # We can use collections.Counter and it's most_common function to extend your list\n",
    "    count.extend(collections.Counter(text_corpus).most_common(vocabulary_size-1))\n",
    "    \n",
    "    \n",
    "    # Define a dictionary called 'dictionary'\n",
    "    # and for every word in 'count', add it to dictionary with a tokenization\n",
    "    # Easiest way is to give it the index of the 'count' variable\n",
    "    \n",
    "    dictionary={}\n",
    "    for i,word in enumerate(count):\n",
    "        dictionary[word[0]]=i\n",
    "    # Make a new list of tokens associated with words    \n",
    "    data = []\n",
    "    # Initialize a counter for 'UNK' values \n",
    "    unk_count = 0\n",
    "    \n",
    "    # For all words in corpus, find the associated token, and append to \n",
    "    # the 'data' variable defined above\n",
    "    for word in text_corpus:\n",
    "        if word in dictionary:\n",
    "            token=dictionary[word]\n",
    "        # If word is not in dictionary, it is 'out of vocabulary'\n",
    "        # So we need to assign it the zero token and\n",
    "        # update the count of the 'UNK' token\n",
    "        else:\n",
    "            token = 0  \n",
    "            unk_count += 1\n",
    "            \n",
    "        # Append token to data \n",
    "        data.append(token)\n",
    "        \n",
    "    # We can now set the count of 'UNK' tokens in the corpus\n",
    "    count[0][1]=unk_count\n",
    "    \n",
    "    # A reverse dictionary takes you from tokens to words\n",
    "    reversed_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "    \n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the helper function to convert the corpus to sequential data\n",
    "vocab_size = len(set(corpus))+1\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(corpus,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sentence was:    rohan was hit by a red bus, so kunal rushed to the scene with a red scarf, but chris and marios did not rush to the scene and instead watched a movie\n"
     ]
    }
   ],
   "source": [
    "# print the original sentence\n",
    "print('The original sentence was:   ',raw_data.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenized form of the sentence is : [7, 8, 9, 10, 1, 2, 11, 12, 13, 14, 3, 4, 5, 15, 1, 2, 16, 17, 18, 6, 19, 20, 21, 22, 3, 4, 5, 6, 23, 24, 1, 25]\n"
     ]
    }
   ],
   "source": [
    "# Print the list of tokenized words associated with this sentence\n",
    "print('The tokenized form of the sentence is :', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformed_sentence:   rohan was hit by a red bus, so kunal rushed to the scene with a red scarf, but chris and marios did not rush to the scene and instead watched a movie\n"
     ]
    }
   ],
   "source": [
    "# Print the sentence transformed from the tokenized list above\n",
    "reformed_sentence=\" \".join([reverse_dictionary[i] for i in data ])\n",
    "print('Reformed_sentence:  ', reformed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking if our function is working al right\n",
    "raw_data.lower()==reformed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams\n",
    "\n",
    "![alt text](https://storage.googleapis.com/public_colab_images/nlp/skip-gram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 4], [22, 19], [13, 2], [12, 1], [22, 4]] [1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# We use the `skipgrams` function from tensorflow.keras to build the training dataset\n",
    "window_size = 2\n",
    "couples, labels = skipgrams(data,window_size)\n",
    "\n",
    "# Separate the target,context pairs as word_target, word_context \n",
    "\n",
    "word_center, word_context = zip(*couples)\n",
    "print(couples[:5], labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Word2Vec using Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Custom\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_45 (InputLayer)          [(None, 1, 26)]      0           []                               \n",
      "                                                                                                  \n",
      " input_46 (InputLayer)          [(None, 1, 26)]      0           []                               \n",
      "                                                                                                  \n",
      " dense_57 (Dense)               (None, 1, 200)       5200        ['input_45[0][0]']               \n",
      "                                                                                                  \n",
      " dense_58 (Dense)               (None, 1, 200)       5200        ['input_46[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_38 (Reshape)           (None, 200)          0           ['dense_57[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_39 (Reshape)           (None, 200)          0           ['dense_58[0][0]']               \n",
      "                                                                                                  \n",
      " dotproduct (Dot)               (None, 1)            0           ['reshape_38[0][0]',             \n",
      "                                                                  'reshape_39[0][0]']             \n",
      "                                                                                                  \n",
      " dense_59 (Dense)               (None, 1)            2           ['dotproduct[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,402\n",
      "Trainable params: 10,402\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We build the sub-model for target words\n",
    "# As a dense layer on a one-hot encoded input without bias term\n",
    "# remember that the dense layer will have activation 'linear'\n",
    "# and number of neurons as the embedding dimension\n",
    "\n",
    "\n",
    "embedding_dim = 200         ### It will be number of neurons in the hidden layer \n",
    "word_model = Sequential()\n",
    "word_model.add((Input(shape=(1,vocab_size))))\n",
    "word_model.add(Dense(embedding_dim,activation='linear',use_bias=False))\n",
    "word_model.add(Reshape((embedding_dim,)))\n",
    "\n",
    "\n",
    "# We build the same for the context words\n",
    "context_model = Sequential()\n",
    "context_model.add((Input(shape=(1,vocab_size))))\n",
    "context_model.add(Dense(embedding_dim,activation='linear',use_bias=False))\n",
    "context_model.add(Reshape((embedding_dim,)))\n",
    "\n",
    "\n",
    "# We use the `tf.keras.layers.dot` which returns the \n",
    "# dot product of two output vectors\n",
    "# Read more here --> https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot\n",
    "dot_product = dot([word_model.output, context_model.output], axes=1,\n",
    "                  normalize=False,name='dotproduct')\n",
    "\n",
    "# We also add a sigmoid to ensure the outputs are between 0 & 1\n",
    "# Simply call a Dense layer and on `dot_product_above`\n",
    "sigmoid_dot_product = Dense(1,activation=\"sigmoid\")(dot_product)\n",
    "\n",
    "# Similar to the model above we create our model with inputs\n",
    "# from `word_model` and `context_model` and the output from \n",
    "# the `dot_product`\n",
    "w2v_model = Model(inputs=[word_model.input, context_model.input], \n",
    "              outputs=sigmoid_dot_product,name='Custom')\n",
    "\n",
    "# Again we run the model summary to ensure we have built the\n",
    "# word2vec architecture correctly\n",
    "w2v_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 200)               5200      \n",
      "                                                                 \n",
      " reshape_40 (Reshape)        (None, 200)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,200\n",
      "Trainable params: 5,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_model = Sequential()\n",
    "#word_model.add((Input(shape=(1,vocab_size))))\n",
    "word_model.add(Dense(embedding_dim,activation='linear',use_bias=False,input_dim=26))\n",
    "word_model.add(Reshape((embedding_dim,)))\n",
    "word_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again we compile the model using binary crossentropy and rmsprop optimizer\n",
    "w2v_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after one batch is 0.70\n"
     ]
    }
   ],
   "source": [
    "# Lets choose a random training sample\n",
    "idx = np.random.randint(0, len(labels))\n",
    "\n",
    "# Using the index we call the input values \n",
    "# NOTE: This time, we will have to one-hot encode the input\n",
    "# in order to comply with our new model\n",
    "# Also, we will have to add one extra dimension to the input\n",
    "# using np.expand_dims in order to avoid warnings from tf.keras API\n",
    "onehot_center =  np.expand_dims(to_categorical(word_center[idx],num_classes=vocab_size).reshape(1,-1),axis=0)\n",
    "onehot_context =  np.expand_dims(to_categorical(word_context[idx],num_classes=vocab_size).reshape(1,-1),axis=0)\n",
    "training_label = np.array(labels[idx],dtype='float32').reshape(1,)\n",
    "\n",
    "# We use the tf.keras `model.train_on_batch` to train on a single batch\n",
    "# for demonstration that our model works\n",
    "# more documentation here -> https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#train_on_batch\n",
    "loss = w2v_model.train_on_batch([onehot_center, onehot_context], training_label)\n",
    "print(f'Loss after one batch is {loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain on bigger corpus\n",
    "\n",
    "Retrain the above, but use a bigger corpus (randomly generated using [this GPT-2 auto-complete model](https://transformer.huggingface.co/doc/arxiv-nlp) and use a larger embedding size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BONUS SEGMENT\n",
    "bigger_corpus = \"\"\"\n",
    "Rohan was hit by a red bus, so Kunal rushed to the scene with a red scarf, \n",
    "but Chris and Marios did not rush to the scene because of the circumstances. \n",
    "Instead they decided to chase after the car. Chris and Marios were also hurt \n",
    "when a red vehicle came running down a ramp in front of the building. \n",
    "Chris and Marios were then able to find a police officer and ran into the back of the building.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Efficient* Word2Vec Skipgram with Negative sampling Model - Using the Embedding Layer\n",
    "\n",
    "Now rebuild the same Word2Vec architecture, but this time use the `tf.keras.layers.Embedding()` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SGNS\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " embedding_2_input (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_3_input (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 200)       5200        ['embedding_2_input[0][0]']      \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 200)       5200        ['embedding_3_input[0][0]']      \n",
      "                                                                                                  \n",
      " reshape_43 (Reshape)           (None, 200)          0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_44 (Reshape)           (None, 200)          0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " dotproduct (Dot)               (None, 1)            0           ['reshape_43[0][0]',             \n",
      "                                                                  'reshape_44[0][0]']             \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 1)            2           ['dotproduct[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,402\n",
      "Trainable params: 10,402\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The following code builds the SGNS word2vec architecture\n",
    "# Use the embedding layer instead of one-hot encoding\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "word_model = Sequential()\n",
    "word_model.add(Embedding(vocab_size,embedding_dim,input_length=1))\n",
    "word_model.add(Reshape((embedding_dim, ))) \n",
    "# We build the same for the context words\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size,embedding_dim,input_length=1))\n",
    "context_model.add(Reshape((embedding_dim, ))) \n",
    "\n",
    "# We use the `tf.keras.layers.dot` which returns the \n",
    "# dot product of two output vectors\n",
    "# Read more here --> https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot\n",
    "dot_product = dot([word_model.output, context_model.output], axes=1,\n",
    "                  normalize=False,name='dotproduct') \n",
    "\n",
    "# We also add a sigmoid to ensure the outputs are between 0 & 1\n",
    "# Simply call a Dense layer and on `dot_product_above`\n",
    "sigmoid_dot_product = Dense(1,activation=\"sigmoid\")(dot_product)\n",
    "\n",
    "# Similar to the model above we create our model with inputs\n",
    "# from `word_model` and `context_model` and the output from \n",
    "# the `dot_product`\n",
    "w2v_model =  Model(inputs=[word_model.input, context_model.input], \n",
    "              outputs=sigmoid_dot_product,name='SGNS') \n",
    "\n",
    "\n",
    "# Again we run the model summary to ensure we have built the\n",
    "# word2vec architecture correctly\n",
    "w2v_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compile the model using binary crossentropy and rmsprop optimizer\n",
    "w2v_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after one batch is 0.69\n"
     ]
    }
   ],
   "source": [
    "# Lets choose a random training sample\n",
    "idx = np.random.randint(0, len(labels))\n",
    "\n",
    "# Using the index we call the input values \n",
    "# NOTE: we process the input to comply with the model\n",
    "# i.e changing dtype and shape\n",
    "center_input = np.array(word_center[idx],dtype='float32').reshape(1,)\n",
    "context_input = np.array(word_context[idx],dtype='float32').reshape(1,)\n",
    "training_label = np.array(labels[idx],dtype='float32').reshape(1,)\n",
    "\n",
    "# We use the tf.keras `model.train_on_batch` to train on a single batch\n",
    "# for demonstration that our model works\n",
    "loss = w2v_model.train_on_batch([center_input, context_input], training_label)\n",
    "print(f'Loss after one batch is {loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "    1.) https://jalammar.github.io/illustrated-word2vec/\n",
    "    2.) Univ.AI NLP course and materials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
